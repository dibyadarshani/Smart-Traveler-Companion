{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spark1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZLTzpssVAbN+PXxpJlruH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLuwWh_C9vBF","executionInfo":{"status":"ok","timestamp":1639925239455,"user_tz":-330,"elapsed":1331,"user":{"displayName":"dibyadarshani patra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17880739956795877420"}},"outputId":"342c8bdf-ede1-4b30-bd64-331d820b833e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}],"source":["from zipfile import ZipFile\n","file_name = \"travel-time-rec-master.zip\"\n","\n","with ZipFile(file_name, 'r') as zip1:\n","  zip1.extractall()\n","  print('Done')"]},{"cell_type":"code","source":["!apt-get update\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n","!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n","\n","!ls\n","\n","import findspark\n","findspark.init()\n","\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate() \n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":757},"id":"n41LXp3W-_te","executionInfo":{"status":"ok","timestamp":1639925295506,"user_tz":-330,"elapsed":54576,"user":{"displayName":"dibyadarshani patra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17880739956795877420"}},"outputId":"639227e0-85ea-424c-c1ac-22c343720d08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [833 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,461 kB]\n","Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [73.9 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,452 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [691 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,821 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,230 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,898 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [12.6 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.6 kB]\n","Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.9 kB]\n","Fetched 13.8 MB in 5s (2,808 kB/s)\n","Reading package lists... Done\n","sample_data\t\t       travel-time-rec-master\n","spark-2.3.1-bin-hadoop2.7      travel-time-rec-master.zip\n","spark-2.3.1-bin-hadoop2.7.tgz\n"]},{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://e3cf5fb37553:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f7f2a85d150>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","# general filtering\n","def filter_user(user_df):\n","    null_age = user_df.ageRange.isnull()\n","    null_gender = user_df.gender.isnull()\n","    null_style = user_df.travelStyle.isnull()\n","    one_thou_pt = (user_df.totalPoints > 200)\n","\n","    user_filtered = user_df[one_thou_pt][~null_age][~null_gender][~null_style]\n","\n","    user_filtered = user_filtered[['username', 'ageRange', 'gender', 'travelStyle']]\n","    return user_filtered\n","\n","\n","def filter_review(review_df):\n","\n","    attraction_only = review_df.type == 'Attractions'\n","    filtered_review_df = review_df[['id', 'username', 'type', 'title', 'text', 'rating', 'taObjectCity']]\n","    filtered_review_df = filtered_review_df[attraction_only]\n","\n","    return filtered_review_df\n","\n","def merge_review_and_user(user_df, review_df):\n","\n","    merged_df = pd.merge(review_df, user_df, on=['username'])\n","\n","    return merged_df\n","\n","def foreign_review_filter(merged_df):\n","\n","    span_mask1 = (merged_df.username == 'AnaS1')\n","    span_mask2 = (merged_df.username == 'DaniLK')\n","    span_mask3 = (merged_df.username == 'Aprile_24')\n","    non_city_mask = (merged_df.taObjectCity == 'California')\n","    non_relevant_mask = (merged_df.taObjectCity == 'Yellowstone National Park')\n","\n","    merged_df = merged_df[~span_mask1][~span_mask2][~span_mask3][~non_city_mask][~non_relevant_mask]\n","\n","    return merged_df\n","\n","\n","\n","def popular_city_list(merged_df):\n","\n","    popular_city = []\n","    for item, value in Counter(merged_df.taObjectCity).items():\n","        if value >= 12:\n","            popular_city.append(item)\n","    return popular_city\n","\n","\n","\n","def filter_final(merged_df, popular_city_list):\n","\n","    final_df = merged_df[merged_df.taObjectCity.isin(popular_city_list)]\n","\n","    return final_df\n","\n","\n","\n","# for user feature matrix\n","\n","\n","def user_feature_filter(final_df):\n","\n","    feature_temp = final_df[['username', 'ageRange', 'gender', 'travelStyle']]\n","    feature_temp = feature_temp.drop_duplicates()\n","\n","    return feature_temp\n","\n","def travel_style(feature_temp):\n","\n","    style_lst = [item.split(', ') for item in feature_temp.travelStyle]\n","    style_serie = pd.Series(style_lst)\n","\n","    feature_temp['new_travel'] = style_serie.values\n","\n","    return feature_temp\n","\n","def travel_matrix(feature_temp):\n","\n","\n","    style_matrix = feature_temp['new_travel'].apply(pd.Series)\n","    style_df = pd.get_dummies(style_matrix.apply(pd.Series). \\\n","                  stack()).sum(level=0). \\\n","                  rename(columns = lambda x : x)\n","\n","    return style_df\n","\n","def age_gender_dummie(feature_temp):\n","\n","    feature_temp = pd.get_dummies(feature_temp, \\\n","                                  columns = ['ageRange', 'gender'])\n","\n","    return feature_temp\n","\n","\n","\n","def combine_all_dummies(user_df, style_df, personality_df):\n","\n","    feature_temp = user_df.join(style_df)\n","\n","    feature_final = feature_temp.drop(['travelStyle', \\\n","                                       'new_travel', \\\n","                                       'gender_male', \\\n","                                       '60+ Traveler', \\\n","                                       'username'], axis =1)\n","\n","    feature_final.reset_index(drop=True, inplace=True)\n","    feature_final1 = feature_final.join(personality_df)\n","\n","    return feature_final1\n","\n","\n","\n","\n","# user big 5 personality scores\n","\n","def user_personality_score_merge(personality_df, user_temp):\n","\n","\n","    with_personality_df = pd.merge(personality_df, user_temp, on = 'username')\n","    only_per_df = with_personality_df.drop(['username', 'user_id'], axis=1 )\n","\n","    return only_per_df\n","\n","\n","def mapping_personality(df):\n","\n","    new_df = df.copy()\n","    for i in range(len((df.columns))):\n","        percentile = np.percentile(df.iloc[:, i], 50)\n","\n","        new_items = np.array([True if item >= percentile else False for item in df.iloc[:, i]])\n","        new_df[str(i)]= new_items\n","\n","    return new_df\n","\n","\n","def cleaning_personality_df(df):\n","\n","\n","    new_df = df.drop(['open', 'cons', 'extra', 'agree','neuro'], axis=1)\n","    new_df.columns = ['open', 'cons', 'extra', 'agree','neuro']\n","\n","    return new_df\n","\n","\n","# prep for clustering\n","\n","\n","def cluster_prep_filter(final_df):\n","\n","    cluster_input_df = final_df.copy()\n","    cluster_input_df = cluster_input_df[['title', 'text', 'taObjectCity']]\n","\n","    return cluster_input_df\n","\n","\n","def grouping_city_title(cluster_input_df):\n","    df_title_comb = cluster_input_df.groupby(['taObjectCity']). \\\n","                            apply(lambda x: ' '. \\\n","                            join(x.title)). \\\n","                            reset_index()\n","\n","    return df_title_comb\n","\n","\n","\n","def grouping_city_text(cluster_input_df):\n","    df_text_comb = cluster_input_df.groupby(['taObjectCity']). \\\n","                                    apply(lambda x: ' '. \\\n","                                    join(x.text)). \\\n","                                    reset_index()\n","    return df_text_comb\n","\n","\n","def merging_content(left, right):\n","\n","    cluster_input_df = left.merge(right, on= 'taObjectCity')\n","    cluster_input_df.columns = ['taObjectCity','title','text']\n","    cluster_input_df.set_index(['taObjectCity'], drop=True, inplace=True)\n","\n","    return cluster_input_df\n","\n","\n","# selected cities from cluster\n","\n","\n","\n","def selected_cities_in_cluster(cluster_df, city_cluster_idx):\n","\n","\n","    cluster2_mask = (cluster_df['cluster_k'] == city_cluster_idx)\n","    up_cluster_df = cluster_df[cluster2_mask]\n","    up_cluster_df.columns = ['cluster_k', 'taObjectCity']\n","\n","    return up_cluster_df\n","\n","def selected_city_df(up_cluster_df, city_df):\n","\n","    selected_df = pd.merge(up_cluster_df, city_df, how = 'left', on = 'taObjectCity')\n","\n","    return selected_df\n"],"metadata":{"id":"Nd9KVh4u_SRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","\n","# traveler profile\n","def load_user_profile(file_path):\n","    df = pd.read_excel(file_path)\n","    return df\n","\n","# traveler articles\n","def load_articles(file_path):\n","    df = pd.read_excel(file_path)\n","    return df\n","\n","# traveler and reviews\n","def load_reviews(file_path):\n","    df = pd.read_excel(file_path)\n","    return df\n","\n","# articles by some traveler\n","def load_personality_scores(file_path):\n","    df = pd.read_excel(file_path)\n","    return df\n"],"metadata":{"id":"sKCJjZtc_9_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prep for spark ALS model\n","\n","import pandas as pd\n","\n","\n","def prep_als_df(final_df):\n","\n","    asl_temp_df = final_df[['username', 'taObjectCity', 'rating']]\n","\n","    return asl_temp_df\n","\n","\n","def unique_user_id(input_df):\n","\n","    user_dict_df = pd.DataFrame(input_df.username.unique(), columns = ['username'])\n","    user_temp = user_dict_df.reset_index()\n","    user_temp = user_temp.rename(columns = {'index':'user_id'})\n","\n","    return user_temp\n","\n","\n","def unique_city_id(input_df):\n","    city_dict_df = pd.DataFrame(input_df.taObjectCity.unique(), columns = ['taObjectCity'])\n","    city_temp = city_dict_df.reset_index()\n","    city_temp = city_temp.rename(columns = {'index':'city_id'})\n","    return city_temp\n","\n","\n","def merging_unique_user_city(left_df, right_df_1, right_df_2):\n","\n","    up_temp_df = pd.merge(left_df, right_df_1, on = 'username')\n","    result_df = pd.merge(up_temp_df, right_df_2, on = 'taObjectCity')\n","\n","    return result_df\n","\n","\n","def utility_matrix(result_df):\n","\n","    agg_dict = {'rating':'median'}\n","    util_matrix = result_df.groupby(['user_id', 'city_id']).agg(agg_dict).reset_index()\n","\n","    return util_matrix\n"],"metadata":{"id":"HdFYDdpSAC5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#general\n","import pandas as pd\n","import numpy as np\n","\n","\n","# spark\n","from pyspark.ml.recommendation import ALS\n","from pyspark.sql.types import *\n","import pyspark\n","from pyspark.sql import SQLContext, Row\n","\n","import sys\n","import os\n","\n","\n","\n","def spark_rdd(util_matrix):\n","    # Build our Spark Session and Context\n","    spark = pyspark.sql.SparkSession.builder.getOrCreate()\n","    sc = spark.sparkContext\n","    #spark, sc\n","    sqlContext = SQLContext(sc)\n","\n","\n","    schema = StructType( [\n","    StructField('user', IntegerType(), True),\n","    StructField('city', IntegerType(), True),\n","    StructField('rating', FloatType(), True)]\n","    )\n","\n","    spark_df = sqlContext.createDataFrame(util_matrix, schema)\n","\n","    return spark_df\n","\n","\n","\n","def ALS_model(spark_df):\n","\n","    #train, test = spark_df.randomSplit([0.85, 0.15], seed=427471138)\n","\n","    als_model = ALS(userCol='user',\n","                    itemCol='city',\n","                    ratingCol='rating',\n","                    nonnegative=True,\n","                    regParam=0.1,\n","                    rank=15\n","                   )\n","\n","    #als_recommender = als_model.fit(train)\n","    als_recommender = als_model.fit(spark_df)\n","\n","    user_factor_df = als_recommender.userFactors.toPandas()\n","    item_factor_df = als_recommender.itemFactors.toPandas()\n","\n","    user_factor_df.to_pickle('user_factor_df.pkl')\n","    item_factor_df.to_pickle('item_factor_df.pkl')\n","\n","\n","\n","if __name__ == '__main__':\n","    # loading dataframe\n","    reviews_file = 'travel-time-rec-master/data/reviews_32618_for_1098_users_with_location.xlsx'\n","    user_path = 'travel-time-rec-master/data/users_full_7034.xlsx'\n","\n","    u_df = load_user_profile(user_path)\n","    r_df = load_reviews(reviews_file)\n","\n","\n","    # filtering dataframe\n","    u_filtered = filter_user(u_df)\n","    r_filtered = filter_review(r_df)\n","    merge_filtered = merge_review_and_user(u_filtered, r_filtered)\n","    merge_filtered = foreign_review_filter(merge_filtered)\n","    pop_city_lst = popular_city_list(merge_filtered)\n","    final_df = filter_final(merge_filtered, pop_city_lst)\n","\n","\n","    # prep df for spark rdd\n","    als_temp_df = prep_als_df(final_df)\n","    user_temp = unique_user_id(als_temp_df)\n","    city_temp = unique_city_id(als_temp_df)\n","    # with username, cityname, rating, user_id, city_id\n","    result_df = merging_unique_user_city(als_temp_df, user_temp, city_temp)\n","    # with user_id, city_id, rating - aggregated by cityid and userid\n","    util_matrix = utility_matrix(result_df)\n","\n","\n","    spark_df = spark_rdd(util_matrix)\n","    ALS_model(spark_df)\n","\n","\n","    print(\"DONE MODELING! \"*5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMVEV56AAK8h","executionInfo":{"status":"ok","timestamp":1639925318595,"user_tz":-330,"elapsed":23098,"user":{"displayName":"dibyadarshani patra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17880739956795877420"}},"outputId":"cb37fa77-b27b-4dbf-edd5-89241deca8dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"]},{"output_type":"stream","name":"stdout","text":["DONE MODELING! DONE MODELING! DONE MODELING! DONE MODELING! DONE MODELING! \n"]}]},{"cell_type":"code","source":["#general\n","import pandas as pd\n","import numpy as np\n","import pickle\n","# cluster\n","\n","from sklearn.cluster import KMeans\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction import text\n","\n","# similarity\n","\n","\n","class TravelModelMain():\n","\n","    def __init__(self, user_df, item_df):\n","\n","        self.user_df = user_df\n","        self.item_df = item_df\n","        # predict --------\n","        #input\n","        #self.k_recommendation = k_recommendation\n","\n","    def cluster_texts(self, corpus):\n","        \"\"\"\n","        Transform texts to Tf-Idf coordinates and cluster texts using K-Means\n","        \"\"\"\n","        #my_additional_stop_words = ['acute', 'good', 'great', 'really', 'just', 'nice', 'like', 'day']\n","        # my_additional_stop_words = ['acute', 'good', 'great', 'really', 'just', 'nice',\n","        #                             'like', 'day', 'beautiful', 'visit', 'time', 'don',\n","        #                             'did', 'place', 'didn', 'did', 'tour', 'sydney','pm',\n","        #                             'lot', '00', 'inside', 'istanbul', 'doesn','going',\n","        #                             'right', '15']\n","        my_additional_stop_words = ['acute', 'good', 'great', 'really',\n","                                    'just', 'nice', 'like', 'day', 'ok',\n","                                    'visit', 'did', 'don', 'place', 'london',\n","                                    'paris','san', 'sydney', 'dubai','diego',\n","                                    'didn', 'fun', 'venice','boston', 'chicago',\n","                                    'tour', 'went', 'time', 'vegas', 'museum',\n","                                    'disney', 'barcelona', 'st', 'pm', 'sf',\n","                                    'worth', 'beautiful', 'la', 'interesting',\n","                                    'inside', 'outside', 'experience', 'singapore',\n","                                    'lot', 'free', 'istanbul', 'food', 'people',\n","                                    'way']\n","        stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n","\n","\n","        vectorizer = TfidfVectorizer(stop_words= stop_words,\n","                                     max_features = 500,\n","                                     lowercase=True)\n","\n","        tfidf_model = vectorizer.fit_transform(corpus)\n","        vectors = tfidf_model.toarray()\n","        cols = vectorizer.get_feature_names()\n","\n","        return (vectors, cols)\n","\n","\n","    def cluster(self, vectors, cols, reviews):\n","        \"\"\"\n","        Cluster vecotirzed reviews and create k books a data frame relating the\n","        k label to the book id\n","        \"\"\"\n","        kmeans = KMeans(4, random_state=10000000).fit(vectors)\n","        k_books = pd.DataFrame(list(zip(list(kmeans.labels_),\n","                                    list(reviews.index))),\n","                                    columns=['cluster_k', 'city_index'])\n","\n","        ''' added code to print centriod vocab - Print the top n words from all centroids vocab\n","        '''\n","        n = 15\n","        centroids = kmeans.cluster_centers_\n","        for ind, c in enumerate(centroids):\n","            #print(ind)\n","            indices = c.argsort()[-1:-n-1:-1]\n","            #print([cols[i] for i in indices])\n","            #print(\"==\"*20)\n","\n","        #print(k_books.head(190))\n","        return k_books\n","\n","\n","    def fit(self, utility_matrix, invert_feature, city_temp, content, reviews):\n","\n","\n","        self.utility_matrix = utility_matrix\n","        self.invert_feature = invert_feature\n","        #print('=========invert_feature=========', invert_feature.tail(10))\n","        print()\n","        self.city_temp = city_temp\n","        #print('=====city_temp=======' ,city_temp.head(80))\n","\n","        vector, cols = self.cluster_texts(content)\n","        self.cluster_df = self.cluster(vector, cols, reviews)\n","\n","\n","################################################################################################\n","\n","\n","\n","    def predict(self, cluster_id, user_id):\n","\n","\n","        up_cluster_df = selected_cities_in_cluster(self.cluster_df, cluster_id)\n","        selected_df = selected_city_df(up_cluster_df, self.city_temp)\n","\n","        final_rating_lst = []\n","\n","        for city in selected_df.city_id:\n","            user_i = user_id\n","            item = city\n","\n","            user_arr = self.user_df.features[user_i]\n","            city_arr = self.item_df[self.item_df.id == item].features.to_numpy()[0]\n","\n","            als_score = np.dot(user_arr, city_arr)\n","\n","            final_sim_score = self.jaccard_sim_score(user_i, item, self.invert_feature, self.utility_matrix)\n","            final_rating = self.overall_rating(als_score, final_sim_score)\n","            final_pair = (final_rating, item)\n","            final_rating_lst.append(final_pair)\n","\n","\n","        rec_cities = self.top_list(final_rating_lst, selected_df)\n","\n","        return rec_cities\n","\n","\n","\n","    def jaccard_sim_score(self, udi, cid, user_matrix, util_matrix):\n","        '''\n","        takes in user(index) and item\n","        returns jaccard similarity score\n","        '''\n","        overall_rating = 0\n","        overall_sim = 0\n","        final_score = 0\n","\n","        filtered_user = util_matrix[util_matrix.city_id == cid]\n","        #print(user_matrix.head(10))\n","        #print(filtered_user)\n","\n","        for user in filtered_user.user_id.values:\n","\n","            #print('***type******', user_matrix[user])\n","            # import pdb; pdb.set_trace()\n","            sim_score = sklearn.metrics.jaccard_score(list(user_matrix[udi].values), list(user_matrix[user].values))\n","            rating = filtered_user[(filtered_user.user_id == user)].rating.values[0]\n","            overall_rating += sim_score * rating\n","            overall_sim +=sim_score\n","\n","        print('***type******', list(user_matrix[udi].values))\n","        final_score = overall_rating / overall_sim\n","\n","        return final_score\n","\n","\n","    def overall_rating(self, als_score ,jacc_sim_score):\n","        alpha = 0.3\n","        beta = 0.7\n","        if als_score == 0:\n","            final_score = jacc_sim_score\n","        else:\n","            final_score = alpha * jacc_sim_score + beta * als_score\n","\n","        return final_score\n","\n","\n","\n","    def top_list(self, final_rating_lst, selected_df):\n","\n","        top_lst = sorted(final_rating_lst, reverse = True)[:3]\n","\n","\n","        rec_items = []\n","        for rating, city in top_lst:\n","            row = selected_df.loc[selected_df['city_id'] == city]\n","            rec_city = row.taObjectCity.values\n","            rec_items.append(rec_city[0])\n","\n","        return rec_items\n"],"metadata":{"id":"VB7DBFlVEOy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","\n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","\n","\n","# loading dataframe\n","reviews_file = 'travel-time-rec-master/data/reviews_32618_for_1098_users_with_location.xlsx'\n","user_file = 'travel-time-rec-master/data/users_full_7034.xlsx'\n","personality_file = 'travel-time-rec-master/data/pers_scores_1098.xlsx'\n","article_file = 'travel-time-rec-master/data/articles_159.xlsx'\n","\n","u_df = load_user_profile(user_file)\n","#df2 = load.load_articles(article_file)\n","r_df = load_reviews(reviews_file)\n","p_df = load_personality_scores(personality_file)\n","\n","\n","# filtering dataframe\n","u_filtered = filter_user(u_df)\n","r_filtered = filter_review(r_df)\n","merge_filtered = merge_review_and_user(u_filtered, r_filtered)\n","merge_filtered = foreign_review_filter(merge_filtered)\n","pop_city_lst = popular_city_list(merge_filtered)\n","#pop_city_lst = gn.popular_city_list(merge_filtered)\n","final_df = filter_final(merge_filtered, pop_city_lst)\n","\n","\n","\n","# prep df for spark rdd\n","als_temp_df = prep_als_df(final_df)\n","user_temp = unique_user_id(als_temp_df)\n","city_temp = unique_city_id(als_temp_df)\n","#print('city_temp:', city_temp.head(120))\n","# with username, cityname, rating, user_id, city_id\n","result_df = merging_unique_user_city(als_temp_df, user_temp, city_temp)\n","# with user_id, city_id, rating - aggregated by cityid and userid\n","util_matrix = utility_matrix(result_df)\n","\n","\n","\n","\n","only_per_df = user_personality_score_merge(p_df, user_temp)\n","personality_matrix_df = mapping_personality(only_per_df)\n","c_personality_matrix_df = cleaning_personality_df(personality_matrix_df)\n","\n","\n","# user-feature matrix\n","feature_temp_0 = user_feature_filter(final_df)\n","feature_temp = travel_style(feature_temp_0)\n","style_df = travel_matrix(feature_temp)\n","feature_temp_1 = age_gender_dummie(feature_temp_0)\n","invert_feature = combine_all_dummies(feature_temp_1, style_df, c_personality_matrix_df).T\n","\n","\n","\n","\n","# clustering\n","#prep\n","cluster_input_df = cluster_prep_filter(final_df)\n","#print('final_df', final_df.head(120))\n","#print('cluster_input_df', cluster_input_df.head(120))\n","df_title_comb = grouping_city_title(cluster_input_df)\n","df_text_comb = grouping_city_text(cluster_input_df)\n","cluster_final = merging_content(df_title_comb, df_text_comb)\n","\n","\n","\n","\n","reviews = cluster_final.title + ' ' + cluster_final.text\n","content = [i for i in reviews]\n","#desire_clusters = 5\n","\n","\n","# ------model--------\n","\n","\n","user_df = pd.read_pickle('user_factor_df.pkl')\n","item_df = pd.read_pickle('item_factor_df.pkl')\n","#travel_m = TravelModelMain(desire_clusters)\n","\n","travel_m = TravelModelMain(user_df, item_df)\n","# ------fit--------\n","#print('+++++invert_feature+++++', invert_feature.tail(10))\n","travel_m.fit(util_matrix, invert_feature, city_temp, content, reviews)\n","\n","pickle.dump(travel_m, open('samp.p', 'wb'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8_NmxMUGauo","executionInfo":{"status":"ok","timestamp":1639925327247,"user_tz":-330,"elapsed":6868,"user":{"displayName":"dibyadarshani patra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17880739956795877420"}},"outputId":"0afe105c-c425-4cb0-f34e-29b1869ad232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]}]}